# -*- coding: utf-8 -*-
"""CyberCrime.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16BFuy3Oi7splnlK7U8pUnBmIENFQYHqY

#Importing Libraries
"""

import re
import pandas as pd
import nltk
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud
import seaborn as sns

df = pd.read_csv('df_m.csv')
pd.set_option('display.max_colwidth', 0)

"""Downloading NLTK data"""

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

"""#Data Cleaning"""

df['text'] = df['text'].apply(lambda text: text.lower())
df['text'] = df['text'].apply(lambda text: re.sub(r'http\S+', '', text))      # Remove URLs
df['text'] = df['text'].apply(lambda text: re.sub(r'@\w+', '', text))         # Remove mentions
df['text'] = df['text'].apply(lambda text: re.sub(r'#\w+', '', text))         # Remove hashtags
df['text'] = df['text'].apply(lambda text: re.sub(r'[^a-zA-Z\s]', '', text))  # Remove special characters
df['text'] = df['text'].apply(lambda text: re.sub(r'\n', '', text))

"""Tabular form after cleaning"""

print("Tabular form after cleaning (lowercasing, removing URLs, and special characters):")
print(df[['text']].head())

"""Visualization after cleaning"""

all_words_cleaning = ' '.join(df['text'])
word_freq_cleaning = nltk.FreqDist(all_words_cleaning.split())
top_words_cleaning = pd.DataFrame(word_freq_cleaning.most_common(20), columns=['Word', 'Frequency'])

"""Barplot after Cleaning"""

plt.figure(figsize=(10, 5))
sns.barplot(x='Frequency', y='Word', data=top_words_cleaning)
plt.title(f'Top 20 Frequent Words after Cleaning')
plt.show()

"""Wordcloud after cleaning"""

wordcloud_cleaning = WordCloud(width=800, height=400, background_color='white').generate(all_words_cleaning)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_cleaning, interpolation='bilinear')
plt.axis('off')
plt.title(f'Word Cloud after Cleaning')
plt.show()

"""#Tokenization"""

df['tokenized_text'] = df['text'].apply(nltk.word_tokenize)

"""Tabular form after tokenization"""

print("\nTabular form after tokenization:")
print(df[['text', 'tokenized_text']].head())

"""Flatten tokens for visualization"""

df['tokenized_text_str'] = df['tokenized_text'].apply(lambda x: ' '.join(x))
all_words_tokenization = ' '.join(df['tokenized_text_str'])
word_freq_tokenization = nltk.FreqDist(all_words_tokenization.split())
top_words_tokenization = pd.DataFrame(word_freq_tokenization.most_common(20), columns=['Word', 'Frequency'])

"""Barplot after tokenization"""

plt.figure(figsize=(10, 5))
sns.barplot(x='Frequency', y='Word', data=top_words_tokenization)
plt.title(f'Top 20 Frequent Words after Tokenization')
plt.show()

"""WordCloud after tokenization"""

wordcloud_tokenization = WordCloud(width=800, height=400, background_color='white').generate(all_words_tokenization)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_tokenization, interpolation='bilinear')
plt.axis('off')
plt.title(f'Word Cloud after Tokenization')
plt.show()

"""#Stopwords removal"""

stop_words = set(stopwords.words('english'))
df['filtered_tokens'] = df['tokenized_text'].apply(lambda x: [word for word in x if word not in stop_words])

"""Flatten tokens for visualization after stopword removal"""

df['filtered_tokens_str'] = df['filtered_tokens'].apply(lambda x: ' '.join(x))
all_words_stopwords = ' '.join(df['filtered_tokens_str'])
word_freq_stopwords = nltk.FreqDist(all_words_stopwords.split())
top_words_stopwords = pd.DataFrame(word_freq_stopwords.most_common(20), columns=['Word', 'Frequency'])

"""Barplot after stopwords removal"""

plt.figure(figsize=(10, 5))
sns.barplot(x='Frequency', y='Word', data=top_words_stopwords)
plt.title(f'Top 20 Frequent Words after Stopwords Removal')
plt.show()

"""Word Cloud after stopwords removal"""

wordcloud_stopwords = WordCloud(width=800, height=400, background_color='white').generate(all_words_stopwords)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_stopwords, interpolation='bilinear')
plt.axis('off')
plt.title(f'Word Cloud after Stopwords Removal')
plt.show()

"""Lemmatization"""

lemmatizer = WordNetLemmatizer()
df['lemmatized_text'] = df['filtered_tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])

""" Flatten tokens for visualization after lemmatization"""

df['lemmatized_text_str'] = df['lemmatized_text'].apply(lambda x: ' '.join(x))
all_words_lemmatization = ' '.join(df['lemmatized_text_str'])
word_freq_lemmatization = nltk.FreqDist(all_words_lemmatization.split())
top_words_lemmatization = pd.DataFrame(word_freq_lemmatization.most_common(20), columns=['Word', 'Frequency'])

"""Barplot after lemmatization"""

plt.figure(figsize=(10, 5))
sns.barplot(x='Frequency', y='Word', data=top_words_lemmatization)
plt.title(f'Top 20 Frequent Words after Lemmatization')
plt.show()

"""Word Cloud after lemmatization"""

wordcloud_lemmatization = WordCloud(width=800, height=400, background_color='white').generate(all_words_lemmatization)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_lemmatization, interpolation='bilinear')
plt.axis('off')
plt.title(f'Word Cloud after Lemmatization')
plt.show()

"""# **TF_IDF**"""

# Join the lemmatized tokens into a single string
df['lemmatized_text'] = df['lemmatized_text'].apply(lambda x: ' '.join(x))

# Step 5: TF-IDF Vectorization after lemmatization
tfidf_vectorizer = TfidfVectorizer(max_features=500)  # Limit to top 500 features (adjust as necessary)
tfidf_matrix = tfidf_vectorizer.fit_transform(df['lemmatized_text'])

# Convert TF-IDF matrix to DataFrame
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# Print TF-IDF matrix as table
print("TF-IDF Matrix:")
print(tfidf_df.head())

# Optional: Display top words in TF-IDF vectorization
word_frequencies = tfidf_df.sum(axis=0).sort_values(ascending=False)
print("\nWord Frequencies (TF-IDF):")
print(word_frequencies.head(10))

"""BAG OF WORDS"""

from sklearn.feature_extraction.text import CountVectorizer

# Step 5: Bag of Words Vectorization after lemmatization
count_vectorizer = CountVectorizer(max_features=500)  # Limit to top 500 features (adjust as necessary)
count_matrix = count_vectorizer.fit_transform(df['lemmatized_text'])

# Convert Bag of Words matrix to DataFrame
bow_df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())

# Print Bag of Words matrix as table
print("Bag of Words Matrix:")
print(bow_df.head())

# Optional: Display top words in Bag of Words vectorization
word_frequencies = bow_df.sum(axis=0).sort_values(ascending=False)
print("\nWord Frequencies (Bag of Words):")
print(word_frequencies.head(10))